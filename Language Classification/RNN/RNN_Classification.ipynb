{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "from numpy import genfromtxt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import keras\n",
    "from keras.datasets import reuters\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import utils as np_utils\n",
    "import pandas as pd\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function for finding the path of the files\n",
    "def findFiles(path): \n",
    "    return glob.glob(path)\n",
    "\n",
    "\n",
    "\n",
    "#Function for turning unicode to ASCII\n",
    "def unicodeToAscii(s,all_letters):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "\n",
    "#Read a file and split it into lines\n",
    "def readLines(filename,all_letters):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    #return [unicodeToAscii(line,all_letters) for line in lines]\n",
    "    return [line for line in lines]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter,all_letters):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line,n_letters,all_letters):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter,all_letters)] = 1\n",
    "    return tensor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return Variable(torch.zeros(1, self.hidden_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to interpret the output of the network, which we know to be a likelihood of each category\n",
    "def categoryFromOutput(output,all_categories):\n",
    "    top_n, top_i = output.data.topk(1) # Tensor out of Variable with .data\n",
    "    category_i = top_i[0][0]\n",
    "    return all_categories[category_i], category_i\n",
    "\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "##Function to get random training samples\n",
    "def randomTrainingExample(all_categories,category_lines,n_letters,all_letters):\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = Variable(torch.LongTensor([all_categories.index(category)]))\n",
    "    line_tensor = Variable(lineToTensor(line,n_letters,all_letters))\n",
    "    return category, line, category_tensor, line_tensor\n",
    "def randomTestingExample(all_categories,category_lines,n_letters,all_letters):\n",
    "    category = randomChoice(all_categories)\n",
    "    line = randomChoice(category_lines[category])\n",
    "    category_tensor = Variable(torch.LongTensor([all_categories.index(category)]))\n",
    "    line_tensor = Variable(lineToTensor(line,n_letters,all_letters))\n",
    "    return category, line, category_tensor, line_tensor\n",
    "\n",
    "#for i in range(10):\n",
    "    #category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#defining a way to train the data \n",
    "def train(rnn,criterion,category_tensor, line_tensor,learning_rate=0.0001):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    if len(line_tensor)==0:\n",
    "        print(\"Zero line occured\")\n",
    "    for i in range(len(line_tensor)):#line_tensor.size()[0]\n",
    "\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.data[0]\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just return an output given a line\n",
    "def evaluate(rnn,line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    n_iters = 100000\n",
    "    print_every = 5000\n",
    "    plot_every = 300\n",
    "    \n",
    "    n_confusion=10000\n",
    "\n",
    "    all_letters = string.ascii_letters + \" .,;'\"\n",
    "    n_letters = len(all_letters)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    # Build the category_lines dictionary, a list of names per language\n",
    "    category_lines = {}\n",
    "    category_lines_test={}\n",
    "    all_categories = []\n",
    "    test_categories=[]\n",
    "    # Read a file and split into lines\n",
    "\n",
    "    for filename in findFiles('langdata_test/*.txt'):\n",
    "        category = filename.split('/')[-1].split('.')[0]\n",
    "        all_categories.append(category)\n",
    "        lines = readLines(filename,all_letters)\n",
    "        category_lines[category] = lines\n",
    "    \n",
    "    \n",
    "    for filename in findFiles('langdata/*.txt'):\n",
    "        category = filename.split('/')[-1].split('.')[0]\n",
    "        test_categories.append(category)\n",
    "        lines = readLines(filename,all_letters)\n",
    "        category_lines_test[category] = lines\n",
    "    \n",
    "\n",
    "    n_categories = len(all_categories)\n",
    "    n_test_categories=len(test_categories)\n",
    "    confusion = torch.zeros(n_categories, n_categories)\n",
    "    confusion_1=torch.zeros(n_test_categories,n_test_categories)\n",
    "    \n",
    "    n_hidden = 100\n",
    "    #Construct the RNN\n",
    "    rnn = RNN(n_letters, n_hidden, n_categories)\n",
    "    \n",
    "    current_loss=0\n",
    "    current_test_loss=0\n",
    "    all_losses=[]\n",
    "    start=time.time()\n",
    "    \n",
    "    for iter in range(1, n_iters + 1):\n",
    "        category, line, category_tensor, line_tensor = randomTrainingExample(test_categories,category_lines,n_letters,all_letters)\n",
    "        output, loss = train(rnn,criterion,category_tensor, line_tensor)\n",
    "        current_loss += loss\n",
    "        \n",
    "#         category,line,category_tensor,line_tensor=randomTestingExample(test_categories,category_lines,n_letters,all_letters)\n",
    "#         output=evaluate(rnn,line_tensor)\n",
    "        \n",
    "        if iter % print_every == 0:\n",
    "            guess, guess_i = categoryFromOutput(output,all_categories)\n",
    "\n",
    "            correct = '✓' if guess == category else '✗ (%s)' % category\n",
    "\n",
    "            print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "\n",
    "        # Add current loss avg to list of losses\n",
    "        if iter % plot_every == 0:\n",
    "            all_losses.append(current_loss / plot_every)\n",
    "            current_loss = 0\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(all_losses)\n",
    "    \n",
    "        # Go through a bunch of examples and record which are correctly guessed\n",
    "    for i in range(n_confusion):\n",
    "        #category, line, category_tensor, line_tensor = randomTrainingExample(all_categories,category_lines,n_letters,all_letters)\n",
    "        category, line, category_tensor, line_tensor = randomTestingExample(test_categories,category_lines_test,n_letters,all_letters)\n",
    "        \n",
    "        output = evaluate(rnn,line_tensor)\n",
    "        guess, guess_i = categoryFromOutput(output,test_categories)\n",
    "        category_i = test_categories.index(category)\n",
    "        confusion[category_i][guess_i] += 1\n",
    "\n",
    "    # Normalize by dividing every row by its sum\n",
    "    for i in range(n_test_categories):\n",
    "        confusion[i] = confusion[i] / confusion[i].sum()\n",
    "\n",
    "    # Set up plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(confusion.numpy())\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + all_categories, rotation=90)\n",
    "    ax.set_yticklabels([''] + all_categories)\n",
    "\n",
    "    # Force label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    # sphinx_gallery_thumbnail_number = 2\n",
    "    plt.show()\n",
    "    return all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5% (2m 16s) 1.6160 Hallo Es sind ca 2000 bilder und speicher 1644 gb / Polish ✗ (German)\n",
      "10000 10% (4m 23s) 1.6113 Claro pero me resulto raro ya que 1451 sigue siendo / Polish ✗ (Spanish)\n",
      "15000 15% (6m 26s) 1.6182 Monica estce que tu veux qu’on te ramène à la / Spanish ✗ (French)\n",
      "20000 20% (8m 29s) 1.6401 da wäre ich mir nicht so sicher Die müssen doch / French ✗ (German)\n",
      "25000 25% (10m 36s) 1.5010 Mnie jest gorąco / Polish ✓\n"
     ]
    }
   ],
   "source": [
    "#space_removal()\n",
    "all_losses=main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(all_losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('/300 iteration')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
